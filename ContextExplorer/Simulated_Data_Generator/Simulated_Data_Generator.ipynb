{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulated Data Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prerequisite\n",
    "* Install VowpalWabbit(VW) by following [this instruction](https://github.com/VowpalWabbit/vowpal_wabbit/wiki/Building)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm\n",
    "from vw_offline_utilities import *\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Generate a Simulated Dataset\n",
    "\n",
    "Based on the config file, we will generate a simulated dataset and save the file for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config File\n",
    "config_file = r'config_data_generator.json'\n",
    "configs = json.load(open(config_file, 'r'))\n",
    "configs = update_params(configs)\n",
    "np.random.seed(configs['random_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "df, context_action_stats = generate_data(**configs)\n",
    "\n",
    "# Increase the leading gap of the best action\n",
    "context_actions = summarize_dataset(df, configs, show_results=False)\n",
    "df = increase_lead(df, context_actions, add_value=configs['increase_winning_margin'])\n",
    "\n",
    "# Finalizing\n",
    "if configs['center']:\n",
    "    df['reward'] = df['reward'] - df['reward'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize data\n",
    "display(df.groupby(list(configs['contexts'].keys())+['action']).agg({'action': 'count', 'reward': 'mean'}).unstack(-1))\n",
    "context_actions = summarize_dataset(df, configs, show_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output data\n",
    "df = df.reset_index().sample(frac=1, random_state=configs['random_state'])\n",
    "df.to_csv(configs['df_file'], index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Transform to DSLogs and Train a VW Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Data Overview\n",
    "\n",
    "In this section, we list the contexts, actions and winning actions for each unique context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names - context, reward and action columns\n",
    "context_cols = list(configs['contexts'].keys())\n",
    "action_col = 'action'\n",
    "reward_col = 'reward'\n",
    "df_cols = context_cols + [action_col, reward_col]\n",
    "idx_cols = context_cols + [action_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape data for the analysis\n",
    "df.dropna(inplace=True)\n",
    "df[action_col] =  df[action_col].astype(str)\n",
    "df = df.sort_values(idx_cols).set_index(idx_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the space of context and action\n",
    "contexts = configs['contexts']\n",
    "actions = [str(x) for x in configs['actions']]\n",
    "action_mapping = {i: a for i, a in enumerate(actions)}\n",
    "display(Markdown('**Contexts**:'), dict(contexts))\n",
    "display(Markdown('**Actions**:'), actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "df_summary = df.reset_index().groupby(context_cols+[action_col])[reward_col].mean().unstack(-1)\n",
    "df_summary.style.apply(lambda x: highlight_optimal(x, is_minimization=False), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 VW Command Lines\n",
    "We will specify the training parameters and commands for VowpalWabbit(VW) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VW Parameters\n",
    "tc = configs['model_parameters']\n",
    "vwc = configs['vw_commands']\n",
    "\n",
    "# VW Commands\n",
    "cmd_train_initial = 'vw --dsjson {0} --cb_explore_adf {1} --cb_type {2} {3} -l {4} -f {5} {6}'.format(\n",
    "    configs['batch_dsjson_path'], vwc['exploration_policy'], vwc['cb_type'], vwc['interactions'], vwc['learning_rate'], configs['model_file'], vwc['other_commands'])\n",
    "cmd_train_continued = 'vw -i {4} --dsjson {0} --cb_explore_adf {1} --cb_type {2} -l {3} -f {4} {5}'.format(\n",
    "    configs['batch_dsjson_path'], vwc['exploration_policy'], vwc['cb_type'], vwc['learning_rate'], configs['model_file'], vwc['other_commands'])\n",
    "cmd_pred_unique_context = 'vw -t -i {0} --dsjson {1} -p {2} -l {3} {4}'.format(\n",
    "    configs['model_file'], configs['context_dsjson_path'], configs['context_pred_path'], vwc['learning_rate'], vwc['other_commands'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Transform Data for VW Modeling\n",
    "\n",
    "VW requires a special data format, DSJson as input. We will transform our tabular data to this format. For details, please visit this [example](https://github.com/VowpalWabbit/vowpal_wabbit/wiki/Conditional-Contextual-Bandit#example-2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique context\n",
    "df_contexts = get_unique_context(df_summary, action_col, reward_col, is_minimization=False)\n",
    "df_contexts_json = transform_dsjson(df_contexts, context_cols, reward_col, action_col, actions, is_minimization=False)\n",
    "export_dsjson(df_contexts_json, configs['context_dsjson_path'])\n",
    "\n",
    "# DSLog preview\n",
    "display(Markdown('**DSLog Preview**'))\n",
    "display(eval(df_contexts_json['output_json'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Train a Model with VW \n",
    "\n",
    "We will train a Contextual Bandit model with VW in this section. We can monitor the accuracy of exploit actions in the mean time. The training logs will be saved in a \\log subfolder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep plot\n",
    "df_batch_accuracy = [np.nan]*(tc['iterations']+1)\n",
    "fig, ax = init_plot(tc['iterations'])\n",
    "\n",
    "# Training\n",
    "trajectory = pd.DataFrame()\n",
    "for i in tqdm(range(tc['iterations'] + 1)):\n",
    "    # Select data\n",
    "    df_batch, control_identifier = select_data(i, df, df_contexts, configs, action_mapping, context_cols, action_col, reward_col)\n",
    "    trajectory = trajectory.append(df_batch)\n",
    "    # Export to dsjson format\n",
    "    df_batch_json = transform_dsjson(df_batch, context_cols, reward_col, action_col, actions, is_minimization=False, other_values=control_identifier)\n",
    "    export_dsjson(df_batch_json, configs['batch_dsjson_path'])\n",
    "    # Plot\n",
    "    df_batch_exploit = df_batch.loc[df_batch['action_prob']==df_batch['prob_list'].apply(lambda x: max(x))]\n",
    "    df_batch_compare = pd.merge(\n",
    "        df_batch_exploit[idx_cols], df_contexts, \n",
    "        how='left', left_on=context_cols, right_on=context_cols, suffixes=['_pred', '_opt'])\n",
    "    df_batch_accuracy[i] = (df_batch_compare[action_col+'_pred']==df_batch_compare[action_col+'_opt']).mean()\n",
    "    plt_dynamic(fig, ax, df_batch_accuracy)\n",
    "    # Train model (When i=iterations, only summarize the model prediction from the last batch without updating the model)\n",
    "    if i!=tc['iterations']:\n",
    "        # Update model\n",
    "        if i == 0:\n",
    "            job = subprocess.Popen(cmd_train_initial)\n",
    "            job.wait()\n",
    "        else:\n",
    "            job = subprocess.Popen(cmd_train_continued)\n",
    "            job.wait() \n",
    "        # Predict with new model\n",
    "        job = subprocess.Popen(cmd_pred_unique_context)\n",
    "        job.wait() \n",
    "        # Keep all inputs by renaming them\n",
    "        new_name = configs['batch_dsjson_path'].replace('.json', '{0}.json'.format(i))\n",
    "        if os.path.exists(new_name):\n",
    "            os.remove(new_name)\n",
    "        os.rename(configs['batch_dsjson_path'], new_name)\n",
    "        # Create control group\n",
    "        if tc['add_control_group']:\n",
    "            create_control_logs(i, df, new_name, configs, actions, context_cols, action_col, reward_col)\n",
    "print('Training logs are save in {0}'.format(os.path.dirname(configs['batch_dsjson_path'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Predictions and Regret\n",
    "\n",
    "We can compare the model predictions with the optimal (ground truth) to validate that the model is taking the best actions.\n",
    "\n",
    "We will also look at the average regret (distance from the optimal) over the training session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the final prediction with the optimal value\n",
    "pred_context = load_pred_context(configs['pred_file'], df_contexts, context_cols, action_mapping)\n",
    "df_compare = pd.merge(df_contexts, pred_context, left_on=context_cols, right_on=context_cols, how='left')\n",
    "df_compare.rename(columns={action_col: 'optimal_action'}, inplace=True)\n",
    "df_compare = df_compare[context_cols + ['optimal_action', 'exploit_action']].astype(str)\n",
    "df_compare.style.apply(lambda x: highlight_suboptimal(x, df_compare['optimal_action'], ['exploit_action']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table above shows that the model predictions match optimal actions in all contexts. \n",
    "\n",
    "Next, we'll look at the regret by context. Regret is defined as the distance between the optimal reward and that from the chosen action. So when then optimal action is learned, there will be no regret. In this particular example, as we used _epsilon 0.2_, which means that we always randomly explore for 20% of the population, so the regret will never be 0 but stay at a low level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regret by iteration\n",
    "regret = get_regrets(trajectory, df_contexts, context_cols, reward_col, vwc['exploration_policy'], is_minimization=False)\n",
    "\n",
    "# Plot Regret by context\n",
    "groups = context_cols + ['exploration', 'n_iteration']\n",
    "plot_data = plot_regrets(regret, groups, rolling_window=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
